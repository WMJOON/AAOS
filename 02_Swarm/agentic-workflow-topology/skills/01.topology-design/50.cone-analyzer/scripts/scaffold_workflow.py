#!/usr/bin/env python3
"""
awt-topology-design cone module scaffold script.
Convergence Cone í”„ë ˆì„ì›Œí¬ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ë””ë ‰í† ë¦¬ì™€ workflow.mdë¥¼ ìƒì„±í•œë‹¤.

v1.0 â†’ v1.1 ë³€ê²½:
  - ë¹„ì„ í˜• DAG í† í´ë¡œì§€ ì§€ì› (--edges)
  - Î¸_GT regimeë³„ ì¢…ë£Œ ì „ëµ ë¶„ë¦¬
  - termination/ ë””ë ‰í† ë¦¬ ì‹ ì„¤ (1ê¸‰ ì‚°ì¶œë¬¼)
  - ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”

Usage:
    # ì„ í˜• ì²´ì¸ (ê¸°ë³¸)
    python3 scaffold_workflow.py \\
        --name my-etl-pipeline \\
        --nodes "classify:converge:L1,extract:converge:L1,reason:diverge:L3,validate:validate:L0" \\
        --output /path/to/output

    # ë¹„ì„ í˜• í† í´ë¡œì§€
    python3 scaffold_workflow.py \\
        --name market-analysis \\
        --nodes "scan:converge:L1,analyze:diverge:L3,synthesize:converge:L1" \\
        --edges "scan->analyze,analyze->analyze:loop,analyze->synthesize" \\
        --d-min 5 \\
        --cooldown 2 \\
        --output /path/to/output
"""

import argparse
import os
import sys
from datetime import datetime, timezone

# â”€â”€ Î¸_GT ë ˆë²¨ â†’ ê¸°ë³¸ê°’ ë§¤í•‘ â”€â”€

VALID_MODES = {"converge", "diverge", "validate"}
VALID_THETA_LEVELS = {"L0", "L1", "L2", "L3", "L4"}
VALID_EDGE_TYPES = {"sequential", "loop", "conditional", "fan_out"}

THETA_DEFAULTS = {
    "L0": {"mode": "validate",  "regime": "convergent",   "model": "Haiku",        "temp": "0",        "d_min": 0, "termination_type": "answer_convergence"},
    "L1": {"mode": "converge",  "regime": "convergent",   "model": "Haiku/Sonnet", "temp": "0~0.3",    "d_min": 0, "termination_type": "answer_convergence"},
    "L2": {"mode": "divergeâ†’converge", "regime": "verificatory", "model": "Sonnet", "temp": "0.5â†’0.2", "d_min": 4, "termination_type": "verification_pass"},
    "L3": {"mode": "diverge",   "regime": "deliberative", "model": "Sonnet/Opus",  "temp": "0.7~1.0",  "d_min": 6, "termination_type": "decision_sufficiency"},
    "L4": {"mode": "diverge",   "regime": "deliberative", "model": "Opus",         "temp": "0.7~1.0",  "d_min": 8, "termination_type": "decision_sufficiency"},
}

# í•˜ìœ„ í˜¸í™˜: ê¸°ì¡´ ë¼ë²¨ â†’ ë ˆë²¨ ë§¤í•‘
LEGACY_THETA_MAP = {
    "very_narrow": "L0", "narrow": "L1", "medium": "L2", "wide": "L3",
}


def error_exit(msg):
    print(f"âŒ Error: {msg}", file=sys.stderr)
    sys.exit(1)


def resolve_theta(raw):
    """Î¸_GT ë¼ë²¨ì„ ì •ê·œí™”. ë ˆê±°ì‹œ ë¼ë²¨ë„ ì§€ì›."""
    if raw in VALID_THETA_LEVELS:
        return raw
    if raw in LEGACY_THETA_MAP:
        return LEGACY_THETA_MAP[raw]
    error_exit(
        f"Unknown Î¸_GT level: '{raw}'. "
        f"Valid: {sorted(VALID_THETA_LEVELS)} or legacy: {sorted(LEGACY_THETA_MAP.keys())}"
    )


def parse_nodes(nodes_str):
    """Parse node string: 'name:mode[:theta],â€¦'"""
    nodes = []
    for i, part in enumerate(nodes_str.split(","), 1):
        tokens = [t.strip() for t in part.strip().split(":")]
        if not tokens or not tokens[0]:
            error_exit(f"Empty node definition at position {i}")

        name = tokens[0]
        mode = tokens[1] if len(tokens) > 1 else None
        theta_raw = tokens[2] if len(tokens) > 2 else None

        # mode ê²€ì¦
        if mode and mode not in VALID_MODES:
            error_exit(
                f"Node '{name}': Invalid mode '{mode}'. Valid: {sorted(VALID_MODES)}"
            )

        # Î¸ ë¯¸ì§€ì • ì‹œ modeì—ì„œ ì¶”ë¡ , modeë„ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        if theta_raw:
            theta = resolve_theta(theta_raw)
        elif mode:
            theta = {"converge": "L1", "diverge": "L3", "validate": "L0"}.get(mode, "L1")
        else:
            theta = "L1"

        defaults = THETA_DEFAULTS.get(theta, THETA_DEFAULTS["L1"])

        # mode ë¯¸ì§€ì • ì‹œ Î¸ì—ì„œ ì¶”ë¡ 
        if not mode:
            mode = defaults["mode"].split("â†’")[0]  # "divergeâ†’converge" â†’ "diverge"

        node = {
            "index": i,
            "id": f"n{i}",
            "name": name,
            "mode": mode,
            "theta": theta,
            "regime": defaults["regime"],
            "model": defaults["model"],
            "temp": defaults["temp"],
            "d_min": defaults["d_min"],
            "termination_type": defaults["termination_type"],
        }
        nodes.append(node)
    return nodes


def parse_edges(edges_str, nodes):
    """Parse edge string: 'a->b,a->c:loop,â€¦'"""
    node_ids = {n["name"] for n in nodes}
    edges = []

    for part in edges_str.split(","):
        part = part.strip()
        if not part:
            continue

        # 'from->to:type' ë˜ëŠ” 'from->to'
        if "->" not in part:
            error_exit(f"Invalid edge format: '{part}'. Expected 'from->to[:type]'")

        arrow_parts = part.split("->")
        src = arrow_parts[0].strip()
        dst_and_type = arrow_parts[1].strip().split(":")
        dst = dst_and_type[0].strip()
        etype = dst_and_type[1].strip() if len(dst_and_type) > 1 else "sequential"

        if src not in node_ids:
            error_exit(f"Edge source '{src}' not found in nodes: {sorted(node_ids)}")
        if dst not in node_ids:
            error_exit(f"Edge target '{dst}' not found in nodes: {sorted(node_ids)}")
        if etype not in VALID_EDGE_TYPES:
            error_exit(f"Invalid edge type '{etype}'. Valid: {sorted(VALID_EDGE_TYPES)}")

        edges.append({"from": src, "to": dst, "type": etype})

    return edges


def infer_linear_edges(nodes):
    """ë…¸ë“œ ìˆœì„œëŒ€ë¡œ ì„ í˜• ì—£ì§€ ìƒì„±."""
    return [
        {"from": nodes[i]["name"], "to": nodes[i + 1]["name"], "type": "sequential"}
        for i in range(len(nodes) - 1)
    ]


def detect_topology(edges):
    """ì—£ì§€ íŒ¨í„´ìœ¼ë¡œ í† í´ë¡œì§€ ìœ í˜• ì¶”ë¡ ."""
    types = {e["type"] for e in edges}
    if "loop" in types:
        return "loop"
    src_counts = {}
    for e in edges:
        src_counts[e["from"]] = src_counts.get(e["from"], 0) + 1
    if any(c > 1 for c in src_counts.values()):
        if "conditional" in types:
            return "conditional"
        return "fan_out"
    return "linear"


def iso_now_z() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


def is_strategy_or_high_risk(workflow_class, risk_tolerance):
    cls = (workflow_class or "").strip().lower()
    risk = (risk_tolerance or "").strip().lower()
    return cls in {"strategy", "high_risk"} or risk == "high"


def make_gate_node(name, index):
    defaults = THETA_DEFAULTS["L0"]
    return {
        "index": index,
        "id": f"n{index}",
        "name": name,
        "mode": "validate",
        "theta": "L0",
        "regime": defaults["regime"],
        "model": defaults["model"],
        "temp": defaults["temp"],
        "d_min": defaults["d_min"],
        "termination_type": defaults["termination_type"],
    }


def ensure_strategy_gate(nodes, edges):
    node_names = [n["name"] for n in nodes]
    next_index = len(nodes) + 1
    for gate in ("C1", "H1", "H2"):
        if gate not in node_names:
            nodes.append(make_gate_node(gate, next_index))
            node_names.append(gate)
            next_index += 1

    edge_keys = {(e["from"], e["to"]) for e in edges}
    required = [("T4", "C1"), ("C1", "H1"), ("H1", "H2")]
    for src, dst in required:
        if src in node_names and dst in node_names and (src, dst) not in edge_keys:
            edges.append({"from": src, "to": dst, "type": "sequential"})
            edge_keys.add((src, dst))

    # ê°€ëŠ¥í•œ ê²½ìš° H2 ë’¤ë¥¼ ê¸°ì¡´ í•©ì„± ë…¸ë“œë¡œ ì—°ê²°í•œë‹¤.
    if "H2" in node_names:
        downstream = None
        for candidate in ("T5", "T8"):
            if candidate in node_names:
                downstream = candidate
                break
        if downstream and ("H2", downstream) not in edge_keys:
            edges.append({"from": "H2", "to": downstream, "type": "sequential"})


# â”€â”€ ë¬¸ì„œ ìƒì„± â”€â”€

def gen_mermaid(nodes, edges):
    """Mermaid DAG ìƒì„±."""
    node_map = {n["name"]: n for n in nodes}
    lines = ["```mermaid", "graph TD"]

    for n in nodes:
        label = f'{n["name"]}\\n[{n["mode"]}|{n["theta"]}|{n["regime"]}]'
        lines.append(f'    {n["id"]}["{label}"]')

    for e in edges:
        src_id = node_map[e["from"]]["id"]
        dst_id = node_map[e["to"]]["id"]
        if e["type"] == "loop":
            lines.append(f'    {src_id} -->|loop: termination_check| {dst_id}')
        elif e["type"] == "conditional":
            lines.append(f'    {src_id} -.->|conditional| {dst_id}')
        else:
            lines.append(f'    {src_id} --> {dst_id}')

    lines.append("```")
    return "\n".join(lines)


def gen_workflow_md(
    wf_name,
    nodes,
    edges,
    d_min_override,
    cooldown,
    strategy_gate_enabled,
    proposal_id,
    visibility_tier,
):
    """workflow.md ìƒì„±."""
    today = datetime.now().strftime("%Y.%m.%d")
    topology = detect_topology(edges)
    mermaid = gen_mermaid(nodes, edges)

    # ë…¸ë“œ ì†ì„± í…Œì´ë¸”
    table_rows = []
    for n in nodes:
        d = d_min_override if d_min_override and n["regime"] == "deliberative" else n["d_min"]
        table_rows.append(
            f'| {n["id"]} | {n["name"]} | `{n["mode"]}` | {n["theta"]} '
            f'| {n["regime"]} | {n["model"]} | {n["temp"]} | {d or "-"} '
            f'| `{n["termination_type"]}` |'
        )

    # Deliberative ë…¸ë“œ ì¢…ë£Œ ìƒì„¸
    delib_sections = []
    for n in nodes:
        if n["regime"] == "deliberative":
            d = d_min_override or n["d_min"]
            delib_sections.append(f"""### {n["id"]}: {n["name"]} â€” Deliberative ì¢…ë£Œ ê·œì¹™

- **Î¸_GT**: {n["theta"]} ({n["regime"]} regime)
- **ì¢…ë£Œ íƒ€ì…**: `decision_sufficiency`
- **D_min**: {d}
- **ì¿¨ë‹¤ìš´ ìœˆë„ìš°(w)**: {cooldown}
- **ì¢…ë£Œ ê³µì‹**:
  ```
  Terminate_deliberative(k) =
    D(k) â‰¥ {d}
    AND orthogonality(n_k, {{n_1..n_{{k-1}}}}) < Îµ  (ì—°ì† {cooldown}íšŒ)
    AND (Î”C(k) < Î´  OR  Î”sem(k) < Ï„)
  ```
- **D_min ë¯¸ë‹¬ ì‹œ**: perspective forcing â†’ `termination/perspective_forcing.md`
- **ì¢…ë£Œ ì„ ì–¸ í˜•ì‹**: â†’ `termination/strategy.md` ì°¸ì¡°
- **ì¢…ë£Œì˜ ì˜ë¯¸**: "ì¶©ë¶„íˆ íƒìƒ‰í•˜ì—¬ íŒë‹¨í•  ìˆ˜ ìˆë‹¤" (NOT "ìµœì„ ì˜ ë‹µì„ ì°¾ì•˜ë‹¤")""")

    delib_detail = "\n\n".join(delib_sections) if delib_sections else "_deliberative ë…¸ë“œ ì—†ìŒ_"

    pf1_block = ""
    if strategy_gate_enabled:
        pf1_block = """## Mandatory Preflight

- PF1: ë©˜íƒˆëª¨ë¸ ë¨¼ì € ì„¸íŒ…í• ê¹Œìš”?

## Strategy/High-Risk Gate

- í•„ìˆ˜ ë…¸ë“œ: C1, H1, H2
- í•„ìˆ˜ ì—£ì§€: T4 -> C1 -> H1
- H1 finalization ì „ web evidence + COWI artifacts ê²€ì¦ í•„ìš”
"""

    return f"""---
artifact_type: workflow_topology_spec
owner_swarm: agentic-workflow-topology
proposal_id: "{proposal_id}"
visibility_tier: "{visibility_tier}"
generated_at: "{iso_now_z()}"
workflow_name: "{wf_name}"
---

# {wf_name}

> Convergence Cone v1.1 ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì •ì˜ì„œ
> Generated: {today}

---

## Overview

- **ì›Œí¬í”Œë¡œìš°**: {wf_name}
- **ë…¸ë“œ ìˆ˜**: {len(nodes)}
- **í† í´ë¡œì§€**: {topology}
- **ì„¤ê³„ ê¸°ì¤€**: Cone Boundary Principle + Orthogonality Principle

---

## Î¸_GT Profile

| ë…¸ë“œ | ì´ë¦„ | Mode | Î¸_GT | Regime | ëª¨ë¸ | Temp | D_min | ì¢…ë£Œ íƒ€ì… |
|------|------|------|------|--------|------|------|-------|----------|
{chr(10).join(table_rows)}

---

## Node Graph

{mermaid}

---

{pf1_block}
---

## Termination Strategy â­

ì¢…ë£ŒëŠ” regimeì— ë”°ë¼ ë‹¤ë¥¸ ì² í•™ì„ ë”°ë¥¸ë‹¤:

| Regime | í•´ë‹¹ ë…¸ë“œ | ì¢…ë£Œ = | ì¢…ë£Œ íƒ€ì… |
|--------|----------|--------|----------|
| Convergent | {', '.join(n['name'] for n in nodes if n['regime']=='convergent') or '-'} | ì •ë‹µ ë„ë‹¬ | `answer_convergence` |
| Verificatory | {', '.join(n['name'] for n in nodes if n['regime']=='verificatory') or '-'} | ê¸°ì¤€ í†µê³¼ | `verification_pass` |
| Deliberative | {', '.join(n['name'] for n in nodes if n['regime']=='deliberative') or '-'} | ì¶©ë¶„í•œ ê·¼ê±° í™•ë³´ | `decision_sufficiency` |

{delib_detail}

ìƒì„¸ ê·œì¹™: â†’ `termination/strategy.md`

---

## Cost Simulation

| ì§€í‘œ | ê¸°ì¡´(ë‹¨ì¼) | ë¶„í•  í›„ | ë³€í™” |
|------|-----------|--------|------|
| ì¢…í•© F1 | _ì¸¡ì • í•„ìš”_ | _ì¸¡ì • í•„ìš”_ | |
| í‰ê·  í† í°/ê±´ | _ì¸¡ì • í•„ìš”_ | _ì¸¡ì • í•„ìš”_ | |
| ì˜ˆìƒ ì›”ë¹„ìš© | _ì¸¡ì • í•„ìš”_ | _ì¸¡ì • í•„ìš”_ | |
| ì¢…ë£Œê¹Œì§€ í‰ê·  ë°˜ë³µ | N/A | _ì¸¡ì • í•„ìš”_ | |
| ì¡°ìœ¨ ì˜¤ë²„í—¤ë“œ | 0 | _ì¸¡ì • í•„ìš”_ | |

ì±„íƒ ê¸°ì¤€: `split_quality / split_cost > baseline Ã— 1.1`

---

## Decision Log

- ë¶„í•  íŒë‹¨ ê·¼ê±°: Î¸_GT ë¶ˆì—°ì† ë³€í™” ì§€ì  {len(nodes)-1}ê°œ ì‹ë³„
- í† í´ë¡œì§€ ì„ íƒ ê·¼ê±°: {topology}
- ë¶„í•  ì±„íƒ ì¡°ê±´: â†’ _ê²€ì¦ í•„ìš”_
"""


def gen_node_md(node, cooldown):
    """ë…¸ë“œë³„ ìƒì„¸ íŒŒì¼ ìƒì„±."""
    return f"""# Node {node["index"]}: {node["name"]}

## ì†ì„±

- **Mode**: `{node["mode"]}`
- **Î¸_GT**: {node["theta"]}
- **Regime**: {node["regime"]}
- **ì¢…ë£Œ íƒ€ì…**: `{node["termination_type"]}`
- **ê¶Œì¥ ëª¨ë¸**: {node["model"]}
- **Temperature**: {node["temp"]}
- **D_min**: {node["d_min"] or "N/A"}

## Logical Axes

_Phase 1ì—ì„œ ì‹ë³„ëœ ì´ ë…¸ë“œì˜ íŒë‹¨ ì¶•ì„ ê¸°ë¡:_

```json
{{
  "logical_axes": [],
  "orthogonality_tracking": true
}}
```

## ì…ë ¥

_ì •ì˜ í•„ìš”_

## ì¶œë ¥

_ì •ì˜ í•„ìš”_

## í”„ë¡¬í”„íŠ¸

â†’ `prompts/node_{node["index"]:02d}_{node["name"]}.prompt` ì°¸ì¡°

## Termination Declaration í˜•ì‹

ì´ ë…¸ë“œëŠ” ì¢…ë£Œ ì‹œ ë°˜ë“œì‹œ ë‹¤ìŒì„ ì¶œë ¥í•´ì•¼ í•œë‹¤:

```json
{{
  "termination_status": "terminate | continue",
  "termination_type": "{node["termination_type"]}",
  "termination_rationale": {{
    "orthogonality_score": "<float>",
    "semantic_expansion_delta": "<float>",
    "decision_sensitivity": "<low | medium | high>",
    "axes_explored": [],
    "axes_remaining_estimate": 0
  }},
  "justification": "<ìì—°ì–´ ì¢…ë£Œ ê·¼ê±°>"
}}
```
"""


def gen_termination_strategy(nodes, d_min_override, cooldown):
    """regimeë³„ ì¢…ë£Œ ì „ëµ ë¬¸ì„œ ìƒì„±."""
    regimes = {}
    for n in nodes:
        regimes.setdefault(n["regime"], []).append(n)

    sections = []

    if "convergent" in regimes:
        names = ", ".join(n["name"] for n in regimes["convergent"])
        sections.append(f"""## Convergent Regime â€” `answer_convergence`

í•´ë‹¹ ë…¸ë“œ: {names}

ì¢…ë£Œ ì¡°ê±´:
- validate (L0): ì¦‰ì‹œ ì¢…ë£Œ (1íšŒ íŒì •)
- converge (L1): Î”sem(k) < Î´ AND k â‰¥ 2 AND confidence > Ï„

ì¢…ë£Œì˜ ì˜ë¯¸: **ì •ë‹µì— ë„ë‹¬í–ˆë‹¤.**""")

    if "verificatory" in regimes:
        names = ", ".join(n["name"] for n in regimes["verificatory"])
        sections.append(f"""## Verificatory Regime â€” `verification_pass`

í•´ë‹¹ ë…¸ë“œ: {names}

ì¢…ë£Œ ì¡°ê±´:
- candidates â‰¥ N_min AND best > Ï„ AND margin(best, second) > Î´

ì¢…ë£Œì˜ ì˜ë¯¸: **ê²€ì¦ ê¸°ì¤€ì„ í†µê³¼í–ˆë‹¤.**""")

    if "deliberative" in regimes:
        names = ", ".join(n["name"] for n in regimes["deliberative"])
        d = d_min_override or max(n["d_min"] for n in regimes["deliberative"])
        sections.append(f"""## Deliberative Regime â€” `decision_sufficiency` â­

í•´ë‹¹ ë…¸ë“œ: {names}

ì¢…ë£Œ ì¡°ê±´:
```
Terminate_deliberative(k) =
  D(k) â‰¥ {d}
  AND orthogonality < Îµ  (ì—°ì† {cooldown}íšŒ)
  AND (Î”C < Î´  OR  Î”sem < Ï„)
  AND decision_sensitivity = low
```

ì¢…ë£Œì˜ ì˜ë¯¸: **ì˜ì‚¬ê²°ì •ì— ì¶©ë¶„í•œ ê·¼ê±°ë¥¼ í™•ë³´í–ˆë‹¤.**
âš ï¸ "ìµœì„ ì˜ ë‹µì„ ì°¾ì•˜ë‹¤"ê°€ ì•„ë‹˜. "ìˆ˜ë ´ ì‹¤íŒ¨"ë¡œ í•´ì„í•˜ì§€ ë§ ê²ƒ.""")

    return "# Termination Strategy\n\n" + "\n\n---\n\n".join(sections)


def gen_terminate_check():
    """ì¢…ë£Œ íŒì • í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿."""
    return """# ì¢…ë£Œ íŒì • í”„ë¡¬í”„íŠ¸ (v1.1)

## 1. ì§êµì„± ì²´í¬ (ëª¨ë“  ë°˜ë³µ ë…¸ë“œ ê³µí†µ)

```
ì´ì „ ë°˜ë³µë“¤ì—ì„œ ì‚¬ìš©ëœ íŒë‹¨ ì¶•: {cumulative_axes}

ì´ë²ˆ ë°˜ë³µì—ì„œ ì‚¬ìš©ëœ íŒë‹¨ ì¶•ì„ ì‹ë³„í•˜ë¼.

íŒì • ê¸°ì¤€:
- ê¸°ì¡´ ì¶•ê³¼ ë…ë¦½ì ì¸ ìƒˆë¡œìš´ íŒë‹¨ ê¸°ì¤€ì¸ê°€?
- ê¸°ì¡´ ì¶•ì˜ ì„ í˜•ê²°í•©(ë‹¨ìˆœ ì¡°í•©)ì´ ì•„ë‹Œê°€?
- ê¸°ì¡´ ì¶•ìœ¼ë¡œëŠ” ë„ë‹¬í•  ìˆ˜ ì—†ëŠ” ê²°ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ”ê°€?

ì¶œë ¥:
- ìƒˆ ì¶•ì´ ìˆë‹¤ë©´: ì¶• ì´ë¦„, ê¸°ì¡´ ì¶•ê³¼ì˜ ì°¨ì´, orthogonality_score (0~1)
- ìƒˆ ì¶•ì´ ì—†ë‹¤ë©´: 'SATURATED', orthogonality_score = 0

ì£¼ì˜: ê°™ì€ ì¶•ì˜ ë‹¤ë¥¸ ê°’ì€ ìƒˆ ì¶•ì´ ì•„ë‹ˆë‹¤.
```

## 2. ê²°ì • ë¯¼ê°ë„ ì²´í¬ (Deliberative regime ì „ìš©)

```
í˜„ì¬ê¹Œì§€ì˜ ì ì • ê²°ë¡ : {í˜„ì¬_ê²°ë¡ }
íƒìƒ‰ë˜ì§€ ì•Šì€ ì ì¬ ì¶•: {ë¯¸íƒìƒ‰_ì¶•_í›„ë³´}

ì§ˆë¬¸: ìœ„ ì ì¬ ì¶• ì¤‘ í•˜ë‚˜ê°€ íƒìƒ‰ë˜ì—ˆì„ ë•Œ,
í˜„ì¬ ê²°ë¡ ì´ ë’¤ì§‘íˆê±°ë‚˜ í¬ê²Œ ë³€í•  ê°€ëŠ¥ì„±ì´ ìˆëŠ”ê°€?

- HIGH: íƒìƒ‰ ê³„ì†. SATURATED ë¬´ì‹œ.
- LOW: ì¢…ë£Œ ì •ë‹¹.
```

## 3. ê²°ì • ì•ˆì •ì„± ì²´í¬

```
ì´ì „ ë°˜ë³µì˜ ì ì • ê²°ë¡ : {ì´ì „_ê²°ë¡ }
ì´ë²ˆ ë°˜ë³µì˜ ì ì • ê²°ë¡ : {í˜„ì¬_ê²°ë¡ }

í•µì‹¬ ì°¨ì´ê°€ ìˆëŠ”ê°€?
- ìˆë‹¤ë©´: ë³€ê²½ëœ ë¶€ë¶„ê³¼ ì´ìœ ë¥¼ ëª…ì‹œ
- ì—†ë‹¤ë©´: 'STABLE'
```
"""


def gen_perspective_forcing():
    """Perspective forcing í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿."""
    return """# Perspective Forcing í”„ë¡¬í”„íŠ¸

D(k) < D_minì¸ë° ì§êµì„±ì´ 0ì— ìˆ˜ë ´í•  ë•Œ ì‚¬ìš©í•œë‹¤.

```
í˜„ì¬ê¹Œì§€ íƒìƒ‰ëœ íŒë‹¨ ì¶•: {cumulative_axes}
í˜„ì¬ D(k) = {í˜„ì¬_ì°¨ì›ìˆ˜}, D_min = {ìµœì†Œ_ì°¨ì›ìˆ˜}

ì•„ì§ íƒìƒ‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆëŠ” ê´€ì :
- ì´í•´ê´€ê³„ìë¥¼ ë°”ê¿”ë³´ë¼ (ê³ ê°â†’ê³µê¸‰ìâ†’ê·œì œìâ†’ì‚¬íšŒ)
- ì‹œê°„ì¶•ì„ ë°”ê¿”ë³´ë¼ (ë‹¨ê¸°â†’ì¤‘ê¸°â†’ì¥ê¸°)
- ì¶”ìƒí™” ìˆ˜ì¤€ì„ ë°”ê¿”ë³´ë¼ (ì „ìˆ â†’ì „ëµâ†’ë¹„ì „)
- ë°˜ëŒ€ ì…ì¥ì—ì„œ ë³´ë¼ (ì°¬ì„±â†’ë°˜ëŒ€â†’ì œ3ì)
- ë„ë©”ì¸ì„ ë°”ê¿”ë³´ë¼ (ê¸°ìˆ â†’ê²½ì œâ†’ë²•ë¥ â†’ìœ¤ë¦¬)

ìœ„ ì¤‘ ê¸°ì¡´ ì¶•ê³¼ ë…ë¦½ì ì¸ ìƒˆë¡œìš´ íŒë‹¨ ì¶•ì„ ì‹ë³„í•˜ë¼.
ì‹ë³„ ë¶ˆê°€í•˜ë©´ 'TRULY_SATURATED'ë¥¼ ë°˜í™˜í•˜ë¼.
```

TRULY_SATURATED ë°˜í™˜ ì‹œ:
- D_minì„ í˜„ì¬ D(k)ë¡œ í•˜í–¥ ì¡°ì •
- Termination Declarationì— D_min í•˜í–¥ ì‚¬ì‹¤ì„ ëª…ì‹œ
- ì´í›„ ì •ìƒ ì¢…ë£Œ íŒì • ì§„í–‰
"""


def gen_quality_criteria():
    """í’ˆì§ˆ ê¸°ì¤€ í…œí”Œë¦¿."""
    return """# í’ˆì§ˆ ê¸°ì¤€ ì •ì˜

## ë…¸ë“œë³„ í’ˆì§ˆ ë©”íŠ¸ë¦­

| ë…¸ë“œ | Mode | Regime | í•µì‹¬ ë©”íŠ¸ë¦­ | ëª©í‘œ |
|------|------|--------|-----------|------|
| _ë…¸ë“œëª…_ | converge | convergent | Precision | â‰¥ 0.90 |
| _ë…¸ë“œëª…_ | diverge | deliberative | Coverage (D/D_min) | â‰¥ 1.0 |
| _ë…¸ë“œëª…_ | diverge | deliberative | Orthogonality at termination | < Îµ |
| _ë…¸ë“œëª…_ | validate | convergent | Accuracy | â‰¥ 0.95 |

## ì „ì²´ ì›Œí¬í”Œë¡œìš° í’ˆì§ˆ

- **ì¢…í•© F1**: _ëª©í‘œ ì„¤ì • í•„ìš”_
- **ë¶„í•  íš¨ìœ¨**: split_quality / split_cost > baseline Ã— 1.1
- **ì¢…ë£Œ ì •ë‹¹ì„±**: ëª¨ë“  ë…¸ë“œì˜ Termination Declarationì´ ì •ìƒ ì¶œë ¥ë˜ëŠ”ê°€?
"""


# â”€â”€ ë©”ì¸ ìŠ¤ìºí´ë“œ â”€â”€

def scaffold(args):
    """ë””ë ‰í† ë¦¬ + íŒŒì¼ ìƒì„±."""
    nodes = parse_nodes(args.nodes)

    # ì—£ì§€ íŒŒì‹± ë˜ëŠ” ì¶”ë¡ 
    if args.edges:
        edges = parse_edges(args.edges, nodes)
    else:
        edges = infer_linear_edges(nodes)

    strategy_gate_enabled = is_strategy_or_high_risk(args.workflow_class, args.risk_tolerance)
    if strategy_gate_enabled:
        ensure_strategy_gate(nodes, edges)

    topology = detect_topology(edges)
    base = os.path.join(args.output, args.name)

    # ë””ë ‰í† ë¦¬ ìƒì„±
    for d in ["nodes", "prompts", "termination", "validation", "analysis"]:
        os.makedirs(os.path.join(base, d), exist_ok=True)

    # workflow.md
    with open(os.path.join(base, "workflow.md"), "w") as f:
        f.write(
            gen_workflow_md(
                args.name,
                nodes,
                edges,
                args.d_min,
                args.cooldown,
                strategy_gate_enabled,
                args.proposal_id,
                args.visibility_tier,
            )
        )

    # ë…¸ë“œ ìƒì„¸
    for n in nodes:
        with open(os.path.join(base, "nodes", f"node_{n['index']:02d}_{n['name']}.md"), "w") as f:
            f.write(gen_node_md(n, args.cooldown))

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    for n in nodes:
        with open(os.path.join(base, "prompts", f"node_{n['index']:02d}_{n['name']}.prompt"), "w") as f:
            f.write(f"# Prompt: {n['name']}\n\n## Mode: {n['mode']}\n## Regime: {n['regime']}\n\n_í”„ë¡¬í”„íŠ¸ ì‘ì„± í•„ìš”_\n")

    # termination/ (v1.1 ì‹ ê·œ)
    with open(os.path.join(base, "termination", "strategy.md"), "w") as f:
        f.write(gen_termination_strategy(nodes, args.d_min, args.cooldown))

    with open(os.path.join(base, "termination", "terminate_check.md"), "w") as f:
        f.write(gen_terminate_check())

    with open(os.path.join(base, "termination", "perspective_forcing.md"), "w") as f:
        f.write(gen_perspective_forcing())

    # validation/
    with open(os.path.join(base, "validation", "quality_criteria.md"), "w") as f:
        f.write(gen_quality_criteria())

    # analysis/ (placeholder)
    with open(os.path.join(base, "analysis", "cone_profile.md"), "w") as f:
        f.write("# Î¸_GT í”„ë¡œíŒŒì¼ë§ ê²°ê³¼\n\n_Phase 1 ì™„ë£Œ í›„ ê¸°ë¡_\n")

    with open(os.path.join(base, "analysis", "cost_simulation.md"), "w") as f:
        f.write("# ë¹„ìš©-í’ˆì§ˆ ì‹œë®¬ë ˆì´ì…˜\n\n_Phase 5 ì™„ë£Œ í›„ ê¸°ë¡_\n")

    # ê²°ê³¼ ì¶œë ¥
    print(f"âœ… Scaffolded: {base}/")
    print(f"   topology: {topology}")
    print(f"   strategy_gate_enabled: {strategy_gate_enabled}")
    print(f"   {len(nodes)} nodes + {len(edges)} edges")
    print(f"   termination/ + validation/ + analysis/")
    print()
    for n in nodes:
        icon = {"deliberative": "âš¡", "verificatory": "ğŸ”", "convergent": "â†’"}.get(n["regime"], "?")
        print(f"   {icon} {n['id']}: {n['name']} [{n['mode']}|{n['theta']}|{n['regime']}] â†’ {n['termination_type']}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Convergence Cone v1.1 ì›Œí¬í”Œë¡œìš° ìŠ¤ìºí´ë”",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # ì„ í˜• ì²´ì¸
  %(prog)s --name etl --nodes "extract:converge:L1,transform:diverge:L3,load:validate:L0"

  # ë¹„ì„ í˜• (ë£¨í”„)
  %(prog)s --name analysis \\
    --nodes "scan:converge:L1,analyze:diverge:L3,report:converge:L1" \\
    --edges "scan->analyze,analyze->analyze:loop,analyze->report"

  # ë ˆê±°ì‹œ ë¼ë²¨ í˜¸í™˜
  %(prog)s --name legacy --nodes "a:converge:narrow,b:diverge:wide"
        """,
    )
    parser.add_argument("--name", required=True, help="ì›Œí¬í”Œë¡œìš° ì´ë¦„")
    parser.add_argument(
        "--nodes", required=True,
        help="ë…¸ë“œ ì •ì˜. í˜•ì‹: 'name:mode[:theta],â€¦' "
             "thetaëŠ” L0~L4 ë˜ëŠ” ë ˆê±°ì‹œ(very_narrow,narrow,medium,wide)")
    parser.add_argument(
        "--edges", default=None,
        help="ì—£ì§€ ì •ì˜ (ì„ íƒ). í˜•ì‹: 'from->to[:type],â€¦' "
             "type: sequential(ê¸°ë³¸), loop, conditional, fan_out. "
             "ë¯¸ì§€ì • ì‹œ ì„ í˜• ì²´ì¸ìœ¼ë¡œ ìë™ ìƒì„±.")
    parser.add_argument("--output", default=".", help="ì¶œë ¥ ê²½ë¡œ (ê¸°ë³¸: í˜„ì¬ ë””ë ‰í† ë¦¬)")
    parser.add_argument("--d-min", type=int, default=None, help="deliberative ë…¸ë“œì˜ D_min ì˜¤ë²„ë¼ì´ë“œ")
    parser.add_argument("--cooldown", type=int, default=2, help="ì¿¨ë‹¤ìš´ ìœˆë„ìš° w (ê¸°ë³¸: 2)")
    parser.add_argument(
        "--workflow-class",
        choices=["strategy", "high_risk", "general"],
        default="general",
        help="ì›Œí¬í”Œë¡œìš° ë¶„ë¥˜ (ê¸°ë³¸: general)",
    )
    parser.add_argument(
        "--risk-tolerance",
        choices=["low", "medium", "high"],
        default="medium",
        help="ë¦¬ìŠ¤í¬ í—ˆìš©ë„ (ê¸°ë³¸: medium)",
    )
    parser.add_argument(
        "--proposal-id",
        default="UNASSIGNED",
        help="proposal identifier for workflow artifacts",
    )
    parser.add_argument(
        "--visibility-tier",
        choices=["must_show", "optional", "internal"],
        default="internal",
        help="visibility tier metadata for workflow artifacts",
    )

    args = parser.parse_args()
    scaffold(args)
